
@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/JKZCK49E/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/KVH3YB8F/1706.html:text/html},
}

@article{munkhdalai_leave_2024,
	title = {Leave {No} {Context} {Behind}: {Efficient} {Infinite} {Context} {Transformers} with {Infini}-attention},
	shorttitle = {Leave {No} {Context} {Behind}},
	url = {http://arxiv.org/abs/2404.07143},
	doi = {10.48550/arXiv.2404.07143},
	abstract = {This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
	month = apr,
	year = {2024},
	note = {arXiv:2404.07143 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/YLBILRQU/Munkhdalai et al. - 2024 - Leave No Context Behind Efficient Infinite Contex.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/EP8H6ZEN/2404.html:text/html},
}

@misc{eldan_tinystories_2023,
	title = {{TinyStories}: {How} {Small} {Can} {Language} {Models} {Be} and {Still} {Speak} {Coherent} {English}?},
	shorttitle = {{TinyStories}},
	url = {http://arxiv.org/abs/2305.07759},
	doi = {10.48550/arXiv.2305.07759},
	abstract = {Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Eldan, Ronen and Li, Yuanzhi},
	month = may,
	year = {2023},
	note = {arXiv:2305.07759 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/3EXEUSEE/Eldan and Li - 2023 - TinyStories How Small Can Language Models Be and .pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/Y4VS3BT3/2305.html:text/html},
}

@misc{choshen_call_2024,
	title = {[{Call} for {Papers}] {The} 2nd {BabyLM} {Challenge}: {Sample}-efficient pretraining on a developmentally plausible corpus},
	shorttitle = {[{Call} for {Papers}] {The} 2nd {BabyLM} {Challenge}},
	url = {http://arxiv.org/abs/2404.06214},
	doi = {10.48550/arXiv.2404.06214},
	abstract = {After last year's successful BabyLM Challenge, the competition will be hosted again in 2024/2025. The overarching goals of the challenge remain the same; however, some of the competition rules will be different. The big changes for this year's competition are as follows: First, we replace the loose track with a paper track, which allows (for example) non-model-based submissions, novel cognitively-inspired benchmarks, or analysis techniques. Second, we are relaxing the rules around pretraining data, and will now allow participants to construct their own datasets provided they stay within the 100M-word or 10M-word budget. Third, we introduce a multimodal vision-and-language track, and will release a corpus of 50\% text-only and 50\% image-text multimodal data as a starting point for LM model training. The purpose of this CfP is to provide rules for this year's challenge, explain these rule changes and their rationale in greater detail, give a timeline of this year's competition, and provide answers to frequently asked questions from last year's challenge.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Choshen, Leshem and Cotterell, Ryan and Hu, Michael Y. and Linzen, Tal and Mueller, Aaron and Ross, Candace and Warstadt, Alex and Wilcox, Ethan and Williams, Adina and Zhuang, Chengxu},
	month = apr,
	year = {2024},
	note = {arXiv:2404.06214 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/D6MRCCKY/Choshen et al. - 2024 - [Call for Papers] The 2nd BabyLM Challenge Sample.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/27NR7A33/2404.html:text/html},
}

@misc{dai_transformer-xl_2019,
	title = {Transformer-{XL}: {Attentive} {Language} {Models} {Beyond} a {Fixed}-{Length} {Context}},
	shorttitle = {Transformer-{XL}},
	url = {http://arxiv.org/abs/1901.02860},
	doi = {10.48550/arXiv.1901.02860},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	note = {arXiv:1901.02860 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/E4ZBLW2R/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/BL6FVMK4/1901.html:text/html},
}

@misc{noauthor_infini-attentioninfini_attentioninfini_attentionpy_nodate,
	title = {infini-attention/infini\_attention/infini\_attention.py at main · vmarinowski/infini-attention},
	url = {https://github.com/vmarinowski/infini-attention/blob/main/infini_attention/infini_attention.py},
	urldate = {2024-04-26},
}

@misc{gupta_continual_2023,
	title = {Continual {Pre}-{Training} of {Large} {Language} {Models}: {How} to (re)warm your model?},
	shorttitle = {Continual {Pre}-{Training} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2308.04014},
	doi = {10.48550/arXiv.2308.04014},
	abstract = {Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and evaluate performance through validation perplexity. We experiment with different pre-training checkpoints, various maximum learning rates, and various warmup lengths. Our results show that while rewarming models first increases the loss on upstream and downstream data, in the longer run it improves the downstream performance, outperforming models trained from scratch\${\textbackslash}unicode\{x2013\}\$even for a large downstream dataset.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Gupta, Kshitij and Thérien, Benjamin and Ibrahim, Adam and Richter, Mats L. and Anthony, Quentin and Belilovsky, Eugene and Rish, Irina and Lesort, Timothée},
	month = sep,
	year = {2023},
	note = {arXiv:2308.04014 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/38XQGIIA/Gupta et al. - 2023 - Continual Pre-Training of Large Language Models H.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/FIMVLB7H/2308.html:text/html},
}

@misc{katharopoulos_transformers_2020,
	title = {Transformers are {RNNs}: {Fast} {Autoregressive} {Transformers} with {Linear} {Attention}},
	shorttitle = {Transformers are {RNNs}},
	url = {http://arxiv.org/abs/2006.16236},
	doi = {10.48550/arXiv.2006.16236},
	abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textasciicircum}2{\textbackslash}right)\$ to \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textbackslash}right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François},
	month = aug,
	year = {2020},
	note = {arXiv:2006.16236 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/Y287AG25/Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/LVILHZJ9/2006.html:text/html},
}

@inproceedings{warstadt_findings_2023,
	address = {Singapore},
	title = {Findings of the {BabyLM} {Challenge}: {Sample}-{Efficient} {Pretraining} on {Developmentally} {Plausible} {Corpora}},
	shorttitle = {Findings of the {BabyLM} {Challenge}},
	url = {https://aclanthology.org/2023.conll-babylm.1},
	doi = {10.18653/v1/2023.conll-babylm.1},
	urldate = {2024-05-03},
	booktitle = {Proceedings of the {BabyLM} {Challenge} at the 27th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and Cotterell, Ryan},
	editor = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and Cotterell, Ryan},
	month = dec,
	year = {2023},
	pages = {1--34},
	file = {Full Text PDF:/Users/laurikeskull/Zotero/storage/SMEEZMZS/Warstadt et al. - 2023 - Findings of the BabyLM Challenge Sample-Efficient.pdf:application/pdf},
}

@misc{ramsauer_hopfield_2021,
	title = {Hopfield {Networks} is {All} {You} {Need}},
	url = {http://arxiv.org/abs/2008.02217},
	doi = {10.48550/arXiv.2008.02217},
	abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	month = apr,
	year = {2021},
	note = {arXiv:2008.02217 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/WSATJ2X5/Ramsauer et al. - 2021 - Hopfield Networks is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/7YDEC4RL/2008.html:text/html},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/ZV25BGIQ/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/Y77VSXYK/2005.html:text/html},
}

@misc{anil_palm_2023,
	title = {{PaLM} 2 {Technical} {Report}},
	url = {http://arxiv.org/abs/2305.10403},
	doi = {10.48550/arXiv.2305.10403},
	abstract = {We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities. When discussing the PaLM 2 family, it is important to distinguish between pre-trained models (of various sizes), fine-tuned variants of these models, and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Anil, Rohan and Dai, Andrew M. and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and Chu, Eric and Clark, Jonathan H. and Shafey, Laurent El and Huang, Yanping and Meier-Hellstern, Kathy and Mishra, Gaurav and Moreira, Erica and Omernick, Mark and Robinson, Kevin and Ruder, Sebastian and Tay, Yi and Xiao, Kefan and Xu, Yuanzhong and Zhang, Yujing and Abrego, Gustavo Hernandez and Ahn, Junwhan and Austin, Jacob and Barham, Paul and Botha, Jan and Bradbury, James and Brahma, Siddhartha and Brooks, Kevin and Catasta, Michele and Cheng, Yong and Cherry, Colin and Choquette-Choo, Christopher A. and Chowdhery, Aakanksha and Crepy, Clément and Dave, Shachi and Dehghani, Mostafa and Dev, Sunipa and Devlin, Jacob and Díaz, Mark and Du, Nan and Dyer, Ethan and Feinberg, Vlad and Feng, Fangxiaoyu and Fienber, Vlad and Freitag, Markus and Garcia, Xavier and Gehrmann, Sebastian and Gonzalez, Lucas and Gur-Ari, Guy and Hand, Steven and Hashemi, Hadi and Hou, Le and Howland, Joshua and Hu, Andrea and Hui, Jeffrey and Hurwitz, Jeremy and Isard, Michael and Ittycheriah, Abe and Jagielski, Matthew and Jia, Wenhao and Kenealy, Kathleen and Krikun, Maxim and Kudugunta, Sneha and Lan, Chang and Lee, Katherine and Lee, Benjamin and Li, Eric and Li, Music and Li, Wei and Li, YaGuang and Li, Jian and Lim, Hyeontaek and Lin, Hanzhao and Liu, Zhongtao and Liu, Frederick and Maggioni, Marcello and Mahendru, Aroma and Maynez, Joshua and Misra, Vedant and Moussalem, Maysam and Nado, Zachary and Nham, John and Ni, Eric and Nystrom, Andrew and Parrish, Alicia and Pellat, Marie and Polacek, Martin and Polozov, Alex and Pope, Reiner and Qiao, Siyuan and Reif, Emily and Richter, Bryan and Riley, Parker and Ros, Alex Castro and Roy, Aurko and Saeta, Brennan and Samuel, Rajkumar and Shelby, Renee and Slone, Ambrose and Smilkov, Daniel and So, David R. and Sohn, Daniel and Tokumine, Simon and Valter, Dasha and Vasudevan, Vijay and Vodrahalli, Kiran and Wang, Xuezhi and Wang, Pidong and Wang, Zirui and Wang, Tao and Wieting, John and Wu, Yuhuai and Xu, Kelvin and Xu, Yunhan and Xue, Linting and Yin, Pengcheng and Yu, Jiahui and Zhang, Qiao and Zheng, Steven and Zheng, Ce and Zhou, Weikang and Zhou, Denny and Petrov, Slav and Wu, Yonghui},
	month = sep,
	year = {2023},
	note = {arXiv:2305.10403 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/F679RS5P/Anil et al. - 2023 - PaLM 2 Technical Report.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/P9KQB3RP/2305.html:text/html},
}

@misc{pope_efficiently_2022,
	title = {Efficiently {Scaling} {Transformer} {Inference}},
	url = {http://arxiv.org/abs/2211.05102},
	doi = {10.48550/arXiv.2211.05102},
	abstract = {We study the problem of efficient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efficiency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32x larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76\% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
	month = nov,
	year = {2022},
	note = {arXiv:2211.05102 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/66IEFA5B/Pope et al. - 2022 - Efficiently Scaling Transformer Inference.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/7XCK4GII/2211.html:text/html},
}

@misc{noauthor_sparse_nodate,
	title = {Sparse {Distributed} {Memory}},
	url = {https://mitpress.mit.edu/9780262514699/sparse-distributed-memory/},
	abstract = {Motivated by the remarkable fluidity of memory the way in which items are pulled spontaneously and effortlessly from our memory by vague similarities to what...},
	language = {en-US},
	urldate = {2024-05-06},
	journal = {MIT Press},
	file = {Snapshot:/Users/laurikeskull/Zotero/storage/S5832GYP/sparse-distributed-memory.html:text/html},
}

@book{kanerva_sparse_1988,
	title = {Sparse {Distributed} {Memory}},
	isbn = {978-0-262-11132-4},
	abstract = {Motivated by the remarkable fluidity of memory the way in which items are pulled spontaneously and effortlessly from our memory by vague similarities to what is currently occupying our attention "Sparse Distributed Memory "presents a mathematically elegant theory of human long term memory.The book, which is self contained, begins with background material from mathematics, computers, and neurophysiology; this is followed by a step by step development of the memory model. The concluding chapter describes an autonomous system that builds from experience an internal model of the world and bases its operation on that internal model. Close attention is paid to the engineering of the memory, including comparisons to ordinary computer memories."Sparse Distributed Memory "provides an overall perspective on neural systems. The model it describes can aid in understanding human memory and learning, and a system based on it sheds light on outstanding problems in philosophy and artificial intelligence. Applications of the memory are expected to be found in the creation of adaptive systems for signal processing, speech, vision, motor control, and (in general) robots. Perhaps the most exciting aspect of the memory, in its implications for research in neural networks, is that its realization with neuronlike components resembles the cortex of the cerebellum.Pentti Kanerva is a scientist at the Research Institute for Advanced Computer Science at the NASA Ames Research Center and a visiting scholar at the Stanford Center for the Study of Language and Information. A Bradford Book.},
	language = {en},
	publisher = {MIT Press},
	author = {Kanerva, Pentti},
	year = {1988},
	keywords = {Computers / Artificial Intelligence / General, Computers / Data Science / Neural Networks, Psychology / Cognitive Psychology \& Cognition, Psychology / General, Self-Help / Personal Growth / Memory Improvement},
}

@misc{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	doi = {10.48550/arXiv.1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv:1409.0473 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/AZRJKKGT/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/D2P6QG88/1409.html:text/html},
}

@misc{cheng_long_2016,
	title = {Long {Short}-{Term} {Memory}-{Networks} for {Machine} {Reading}},
	url = {http://arxiv.org/abs/1601.06733},
	doi = {10.48550/arXiv.1601.06733},
	abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
	month = sep,
	year = {2016},
	note = {arXiv:1601.06733 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/956VEGAC/Cheng et al. - 2016 - Long Short-Term Memory-Networks for Machine Readin.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/KLZVZ2DH/1601.html:text/html},
}

@misc{schlag_learning_2021,
	title = {Learning {Associative} {Inference} {Using} {Fast} {Weight} {Memory}},
	url = {http://arxiv.org/abs/2011.07831},
	doi = {10.48550/arXiv.2011.07831},
	abstract = {Humans can quickly associate stimuli to solve problems in novel contexts. Our novel neural network model learns state representations of facts that can be composed to perform such associative inference. To this end, we augment the LSTM model with an associative memory, dubbed Fast Weight Memory (FWM). Through differentiable operations at every step of a given input sequence, the LSTM updates and maintains compositional associations stored in the rapidly changing FWM weights. Our model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Schlag, Imanol and Munkhdalai, Tsendsuren and Schmidhuber, Jürgen},
	month = feb,
	year = {2021},
	note = {arXiv:2011.07831 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/MQVE4IHJ/Schlag et al. - 2021 - Learning Associative Inference Using Fast Weight M.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/NIFPSWJD/2011.html:text/html},
}

@misc{clevert_fast_2016,
	title = {Fast and {Accurate} {Deep} {Network} {Learning} by {Exponential} {Linear} {Units} ({ELUs})},
	url = {http://arxiv.org/abs/1511.07289},
	doi = {10.48550/arXiv.1511.07289},
	abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10\% classification error for a single crop, single model network.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, Sepp},
	month = feb,
	year = {2016},
	note = {arXiv:1511.07289 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/KGXI8VTA/Clevert et al. - 2016 - Fast and Accurate Deep Network Learning by Exponen.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/S3NI7QQD/1511.html:text/html},
}

@misc{krotov_dense_2016,
	title = {Dense {Associative} {Memory} for {Pattern} {Recognition}},
	url = {http://arxiv.org/abs/1606.01164},
	doi = {10.48550/arXiv.1606.01164},
	abstract = {A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.},
	urldate = {2024-05-21},
	publisher = {arXiv},
	author = {Krotov, Dmitry and Hopfield, John J.},
	month = sep,
	year = {2016},
	note = {arXiv:1606.01164 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/QCH9NAS4/Krotov and Hopfield - 2016 - Dense Associative Memory for Pattern Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/LK73WPRC/1606.html:text/html},
}

@article{demircigil_model_2017,
	title = {On a {Model} of {Associative} {Memory} with {Huge} {Storage} {Capacity}},
	volume = {168},
	issn = {00224715},
	url = {https://go.gale.com/ps/i.do?p=AONE&sw=w&issn=00224715&v=2.1&it=r&id=GALE%7CA497479694&sid=googleScholar&linkaccess=abs},
	abstract = {{\textless}em{\textgreater}Gale{\textless}/em{\textgreater} Academic OneFile includes On a Model of Associative Memory with Huge Storage Capa by Mete Demircigil, Judith Heusel, Matthia. Click to explore.},
	language = {English},
	number = {2},
	urldate = {2024-05-21},
	journal = {Journal of Statistical Physics},
	author = {Demircigil, Mete and Heusel, Judith and Lowe, Matthias and Upgang, Sven and Vermet, Franck},
	month = jul,
	year = {2017},
	note = {Publisher: Springer},
	pages = {288--300},
	file = {Snapshot:/Users/laurikeskull/Zotero/storage/THA7F9HK/i.html:text/html},
}

@misc{ramsauer_hopfield_2021-1,
	title = {Hopfield {Networks} is {All} {You} {Need}},
	url = {http://arxiv.org/abs/2008.02217},
	doi = {10.48550/arXiv.2008.02217},
	abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
	urldate = {2024-05-21},
	publisher = {arXiv},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	month = apr,
	year = {2021},
	note = {arXiv:2008.02217 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/IUMNRXKS/Ramsauer et al. - 2021 - Hopfield Networks is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/EST898B2/2008.html:text/html},
}

@misc{shen_efficient_2024,
	title = {Efficient {Attention}: {Attention} with {Linear} {Complexities}},
	shorttitle = {Efficient {Attention}},
	url = {http://arxiv.org/abs/1812.01243},
	doi = {10.48550/arXiv.1812.01243},
	abstract = {Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github.com/cmsflash/efficient-attention.},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
	month = jan,
	year = {2024},
	note = {arXiv:1812.01243 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, I.2.10, I.2.6, I.4.6, I.4.8},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/Z45X5HWN/Shen et al. - 2024 - Efficient Attention Attention with Linear Complex.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/AGLN73T4/1812.html:text/html},
}

@misc{hsieh_ruler_2024,
	title = {{RULER}: {What}'s the {Real} {Context} {Size} of {Your} {Long}-{Context} {Language} {Models}?},
	shorttitle = {{RULER}},
	url = {http://arxiv.org/abs/2404.06654},
	doi = {10.48550/arXiv.2404.06654},
	abstract = {The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the "needle") from long distractor texts (the "haystack"), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark RULER with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. Moreover, RULER introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate ten long-context LMs with 13 representative tasks in RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only four models (GPT-4, Command-R, Yi-34B, and Mixtral) can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement as we increase input length and task complexity. We open source RULER to spur comprehensive evaluation of long-context LMs.},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
	month = apr,
	year = {2024},
	note = {arXiv:2404.06654 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/laurikeskull/Zotero/storage/KWWYRTSA/Hsieh et al. - 2024 - RULER What's the Real Context Size of Your Long-C.pdf:application/pdf;arXiv.org Snapshot:/Users/laurikeskull/Zotero/storage/2US8T5MF/2404.html:text/html},
}
