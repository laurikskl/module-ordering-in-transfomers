\section{Methodology, Background, Problem Description}
Choose one that fits your research best:
\subsection{Background}
% Typically in general research articles, the second section contains a description of the research methodology, explaining what you, the researcher, 
% is doing to answer the research question(s), and why you have chosen this method.
% For purely analytical work this is a description of the data collection or experimental setup on how to test the hypothesis, with a motivation.

% In any case this section includes references to necessary background information.
% For a survey paper this includes the method of how you arrived at the set of papers included in the survey.


we parameterize the memory with an associative matrix (Schlag et al., 2020). 
LEARNING ASSOCIATIVE INFERENCE USING FAST WEIGHT MEMORY Imanol Schlag
% This is often described as an auto-associative type of memory as it reconstructs a previously stored pattern that mostly resembles the current pattern.
% (Dense Associative Memory for Pattern Recognition Dmitry Krotov,) 
% (On a Model of Associative Memory with Huge Storage CapacityAuthors: Mete Demircigil, Judith Heusel, Matthias Lowe, Sven Upgang and Franck Vermet)
% (Hopfield Networks is All You Need Hubert Ramsauer)

A much less studied variation is the hetero-associative memory (see e.g. Kosko (1988)) where the retrieved pattern is different from the input pattern. This is more relevant for our use case.
We aim to train an LSTM to construct, maintain, and edit its associative memory. The ability to edit Hopfield networks partially is not very well studied. For this reason, we employ a simple (multi-)linear
hetero-associative memory as it is more closely related to the theory of TPRs (whose manipulation is well understood) and because the association is retrieved in a single step.
Our FWM is a fast-changing, multi-linear map which is controlled by a slowly-changing, non-linear LSTM. The slow weights of the LSTM are regular NN weights which 
are updated during training by gradient descent. In contrast, the fast weights of the FWM are updated by the LSTM at every step of the input sequence through a Hebb-like differentiable mechanism. 
This allows the FWM function to change rapidly even during testing—hence the name fast weights. Along with updating the fast weights, 
the LSTM also generates a memory query which is used to retrieve information that was previously stored. The retrieved information then becomes part of the model’s output.


The purpose of writing to memory is to learn a context-specific association between the input pattern k1 ⊗ k2 and the output pattern v. 
The usage of the tensor-product in the input pattern factorises the the representational space which guarantees unique orthogonal vector representations for novel key pairs.


(Bart Kosko. Bidirectional associative memories.)

