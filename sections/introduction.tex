\section{Introduction}
% **The Problem**
% - LLMs are really nice to use for summarizing long texts or giving them documentation that they have not been trained on
% - Increasing context length makes the costs scale quadratically
% - Infini-attention promises infinite context length for a finite cost.
% - They propose a compressive memory,  but can it really hold infinite information?

% **Existing Approaches**
% - They do benchmarks like needle in a haystack 
% - they find a compressive ratio of 114x

% **The Gap**
% - How does the compressive memory scale depending on its size?

Transformers have revolutionized natural language processing, yet they grapple with limitations due to fixed-size context windows,
 restricting their capacity to process lengthy documents and grasp intricate dependencies outside these windows.
  Enhanced models such as Transformer-XL and BigBird have addressed these issues through sparse attention mechanisms and cacheing Key-Value matrices,
   yet challenges persist in terms of scalability and computational efficiency. 

In response to these ongoing challenges, Munkhdalai et al. introduced the concept of Infini-attention, an innovative approach allowing efficient management
 of infinitely long contexts \cite{munkhdalai_leave_2024}.
  Infini-attention integrates a compressive memory into the transformer architecture, allowing for a dynamic and scalable handling
   of extended contexts without a proportional increase in computational demands. This mechanism not only enhances memory efficiency
    but also fosters continuous contextual understanding over extended sequences.

While Infini-attention has demonstrated promising results in fine-tuning scenarios, this study proposes to explore its integration at an
 earlier stageâ€”during model pre-training. This approach hypothesizes that initiating Infini-attention in the pre-training phase could
  leverage its full potential, thereby embedding a deeper contextual understanding from the outset. Specifically,
   this research seeks to (1) evaluate the impact of Infini-attention when introduced in pre-training versus during fine-tuning, 
   and (2) examine the relationship between the context length a transformer can handle and the capacity of its compressive infini-attention memory.

The insights gained from this study could significantly influence future strategies for deploying transformer models across various real-world applications,

 Furthermore, this research will also shed light on the extent to which the compressive memory component of Infini-attention can compensate for shorter local context lengths. 
 By determining how effectively compressive memory can stand in for direct local context interactions, the findings may offer valuable guidelines for
  optimizing transformer configurations, particularly in applications where extending the context window is constrained by computational or memory resources.
  This would potentially lead to more robust, context-aware systems capable of handling complex tasks such as document summarization and legal analysis more effectively.


\end{document}